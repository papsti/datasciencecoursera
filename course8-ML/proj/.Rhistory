## Get raw data
training <- read.csv("data/pml-training.csv")
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
data <- training %>%
filter(new_window == "no")
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
## Also remove cols with NA
data <- training %>%
filter(new_window == "no") %>%
na.omit()
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
## Also remove cols with NA
data <- training %>%
filter(new_window == "no") %>%
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x)
head(data)
## Get raw data
training <- read_csv("data/pml-training.csv")
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
## Also remove cols with NA
data <- training %>%
filter(new_window == "no") %>%
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x)
head(data)
## Training data
data <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
head(data)
## Training data
data <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
head(data)
## Training data
training <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
## Testing data
data <- testing %>%
)
## Testing data
data <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
## Testing data
data <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
head(data)
testing <- read_csv("data/pml-testing.csv")
## Testing data
data <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
## Testing data
testing <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
colnames(training)
colnames(training) == colnames(testing)
colnames(testing)[-1]
head(testing["problem_id"])
colnames(testing)
library(tidyverse)
## Get raw data
training <- read_csv("data/pml-training.csv")
testing <- read_csv("data/pml-testing.csv")
## Training data
training <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
## Testing data
testing <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
## Load data
training <- read_csv("data/pml-training.csv")
## Training data
training <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
library(tidyverse)
## Load data
training <- read_csv("data/pml-training.csv")
testing <- read_csv("data/pml-testing.csv")
## Load data
training <- read_csv("data/pml-training.csv")
## Training data
training <- training %>%
## Filter training to remove rows with summary stats
## (none to predict in the test set so probably not relevant)
filter(new_window == "no") %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
training
## Testing data
testing <- testing %>%
## Remove cols where all vals are NA
map(~.x) %>%
discard(~all(is.na(.x))) %>%
map_df(~.x) %>%
## Remove cols irrelevant to prediction
select(-c(X1, user_name,
raw_timestamp_part_1, raw_timestamp_part_2,
cvtd_timestamp,
new_window, num_window))
head(training)
names(getModelInfo()).
names(getModelInfo())
library(caret)
names(getModelInfo()).
names(getModelInfo())
library(randomForest)
?rfcv
rfcv
library(tidyverse)
library(randomForest)
rfcv
head(training)
head(training[,-53])
cv <- rfcv(training[, -53], as.factor(training[53]))
head(raining[, -53])
head(training[, -53])
cv <- rfcv(training[, -53], as.factor(training[,53]))
head(as.factor(training[,53]))
head(training,[53])
head(training[],53])
head(training[,53])
nrow(training)
as.factor(training[,53])
training[,53]
typeof[training[,53]]
train_x <- training[, -53]
train_y <- training[, 53]
cv <- rfcv(training[, -53], training[, 53])
cv <- rfcv(as.data.frame(training[, -53]),
training[, 53])
cv <- rfcv(as.data.frame(training[, -53]),
as.vector(training[, 53]))
cv <- rfcv(training[, -53],
training[, 53])
cv <- rfcv(training[, -53],
as.vector(training[, 53]))
typeof(training[, -53])
typeof(training[, 53])
typeof(as.vector(training[, 53]))
typeof(c(training[, 53]))
train <- as.data.frame(training)
typeof(train)
train <- as.matrix(training)
typeof(train)
head(train)
rm(train)
rm(list = c(train_x, train_y))
rm(train_x, train_y)
train <- as.data.frame(training)
head(select(train, -classe))
cv <- rfcv(select(train, -classe),
train$classe)
as.factor(train$classe)
cv <- rfcv(select(train, -classe),
as.factor(train$classe))
cross_val <- rfcv(trainx = select(training, -classe),
trainy = as.factor(training$classe))
## Change training tibble to dataframe for compatibility with
## rfcv()
training <- as.data.frame(training)
cross_val <- rfcv(trainx = select(training, -classe),
trainy = as.factor(training$classe))
cross_val
## Plot estimated OOS error from cross-validation as a function of the number of predictors
cv_error <- as.data.frame(n_var = cross_val$n.var,
error = cross_val$error.cv)
cross_val$n.var
cross_val$error.csv
cross_val$error.cv
as.numeric(cross_val$error.cv)
## Plot estimated OOS error from cross-validation as a function of the number of predictors
cv_error <- as.data.frame(n_var = cross_val$n.var,
error = as.numeric(cross_val$error.cv))
## Plot estimated OOS error from cross-validation as a function of the number of predictors
cv_error <- as.data.frame(n_var = cross_val$n.var,
error = as.numeric(cross_val$error.cv))
typeof(cross_val$n.var)
typeof(cross_val$error.cv)
## Plot estimated OOS error from cross-validation as a function of the number of predictors
cv_error <- data.frame(n_var = cross_val$n.var,
error = as.numeric(cross_val$error.cv))
p <- gg_plot(cv_error, aes(x = n_var, y = error))
library(tidyverse)
p <- gg_plot(cv_error, aes(x = n_var, y = error))
p <- ggplot(cv_error, aes(x = n_var, y = error))
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line()
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point()
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
scale_y_log10() +
geom_line() +
geom_point()
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
error.cv
cross_val$error.cv
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(x = 6, y = error[4], col = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(x = 6, y = y[4], col = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(ase(x = 6, y = y[4], col = "dodgerblue")) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = y[4], col = "dodgerblue")) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4], col = "dodgerblue")) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4], colour = "dodgerblue")) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+2, y = error[4]*1.25,
label = paste0("(6, ", error[4], ")")))
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+2, y = error[4]*1.25,
label = paste0("(6, ", error[4], ")"))) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+2, y = error[4]*1.25,
label = paste0("(6, ", round(error[4], 4), ")"))) +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+2, y = error[4]*1.25,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+2, y = error[4]*2,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+4, y = error[4]*3,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+6, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+5, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
importance(cross_val)
rfcv
## Set seed
set.seed(1346)
## Change training tibble to dataframe for compatibility with rfcv()
training <- as.data.frame(training)
cross_val <- rfcv(trainx = select(training, -classe),
trainy = as.factor(training$classe))
## Plot estimated OOS error from cross-validation as a function of the number of predictors
cv_error <- data.frame(n_var = cross_val$n.var,
error = as.numeric(cross_val$error.cv))
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+5, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+5, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 5), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+5, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 4), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
cross_val$error.cv
p <- ggplot(cv_error, aes(x = n_var, y = error)) +
geom_line() +
geom_point() +
geom_point(aes(x = 6, y = error[4]), colour = "dodgerblue") +
geom_text(aes(x = 6+5, y = error[4]*2.5,
label = paste0("(6, ", round(error[4], 3), ")")),
colour = "dodgerblue") +
xlab("Number of predictors") +
ylab("5-fold cross-validation error")
print(p)
full_model <- randomForest(classe ~ ., data = training)
typeof(training)
class(training)
full_model <- randomForest(classe ~ .-class, data = training)
full_model <- randomForest(classe ~ ., data = training)
training$classe <- as.factor(training$classe)
full_model <- randomForest(classe ~ ., data = training)
importance(full_model)
importance(full_model, type = 1)
full_model <- randomForest(classe ~ ., data = training,
importance = TRUE)
importance(full_model)
head(importance(full_model))
sort(importance(full_model))
sort(importance(full_model)$MeanDecreaseAccuracy)
head(importance(full_model))
class(importance(full_model))
class(importance(full_model)[,1])
importance(full_model[,1])
importance(full_model)[,1]
sort(importance(full_model)[,6])
sort(importance(full_model)[,6], desc = TRUE)
sort(importance(full_model)[,6], decreasing = TRUE)
sort(importance(full_model)[,6], decreasing = TRUE)[1:6]
names(sort(importance(full_model)[,6], decreasing = TRUE))[1:6]
